[{"authors":["admin"],"categories":null,"content":"I am currently working with Makarand Tapaswi and Marc Law on problems related to Long-tail distribution in an image classification setting.\nI am also a research Intern at IIT Hyderabad\u0026lsquo;s Machine Learning and Vision Lab advised by Dr Vineeth N Balasubramanian where I work on understanding machine learning models that can learn incrementally.\n  --  2019-2021\n--   2015-2019\n--   For a more consolidated version of this website, take a look at my CV\n","date":1633919212,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1633919212,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently working with Makarand Tapaswi and Marc Law on problems related to Long-tail distribution in an image classification setting.\nI am also a research Intern at IIT Hyderabad\u0026lsquo;s Machine Learning and Vision Lab advised by Dr Vineeth N Balasubramanian where I work on understanding machine learning models that can learn incrementally.","tags":null,"title":"Rahul Vigneswaran","type":"authors"},{"authors":["Rahul Vigneswaran"],"categories":["Summary"],"content":" Datasets\n CIFAR-LT 100 ImageNet-LT Places-LT iNaturalist18    ##### Summary --  Experimental Setup\n   Dataset Architecture     CIFAR-LT 100 ResNet-32   ImageNet-LT ResNeXt-50   Places-LT    iNaturalist18 ResNeXt-50      ##### Summary --  Updates\n [Oct 21] Started adding CVPR, ICLR 21 papers.    Table of Contents  2021  CVPR 21  Improving Calibration for Long-Tailed Recognition Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification Disentangling Label Distribution for Long-tailed Visual Recognition [LADE] MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition       2021 CVPR 21 Improving Calibration for Long-Tailed Recognition Links: arXiv\n Datasets\n CIFAR-LT 10/100 ImageNet-LT Places-LT iNaturalist18    Summary  High Expected Calibration Error (ECE) for networks trained on LT datasets  Mix-up (Takes care of CNN)  Mixup in 1st stage of decouple helps Reduces the weight norm of head and increases tail class weight norm       Label aware smoothing (Takes care of Classifier)  Higher smoothing factor for head and lower for tail  They try three types - Concave, Linear, Convex       cRT has better representation ability and LWS has better generalization property  They combine both together in the second stage        (Problem) Dataset bias/domain shift when decoupling  In 2-stage training, stage-1 is trained on a different domain (sampler) than stage-2 -\u0026gt; Domain shift. (Solution) Batch Normalization  During stage 2, update the running mean and variance but don’t change the trainable params in BN layer     Notes:  cRT:  LWS Works better for large scale datasets compared to cRT.      Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification Links: arXiv\n Experimental Setup\n   Dataset Architecture     CIFAR-LT 10/100 ResNet-32   iNaturalist18 ResNet-50      Summary  Main strategy:  Pull similar samples together AND Push dissimilar samples apart (Supervised contrastive learning (SC))   Hybrid network (Transition from 1 -\u0026gt; 2 with a curriculum) (!! Is this not similar to BBN? Yes but with two different loss instead of samplers)  Features -\u0026gt; Constrastive Loss (SC)  Input : Labels are either positive or negative. Backbone : Common ResNet Projection head : Non-linear embedding layer (with 1 hidden layer) -\u0026gt; $l_2$ normalize the embedding Loss : Supervised contrastive learning loss ($L_{SCL}$)  (Problem) Extra memory consumption problem : We need to use both distance to the +ves and -ves. The number -ves would be high which makes it essential to keep alot of information in the memory. (Solution) Propose Prototypical supervised constrastive learning (PSC)  Pull towards prototypes of its own class and push away from the prototypes of all other classes.  Multiple Prototype Contrastive Learning (MPSC): While PSC uses only one prototype per class. (BUT THEY DONT SHOW ANY RESULTS ON THIS! Future work.)         Classifier -\u0026gt; Cross-entropy (CE)  Input: Usual labels Backbone : Common ResNet Projection head : Linear embedding layer (no hidden layer) Loss : Cross Entropy ($L_{CE}$)      Disentangling Label Distribution for Long-tailed Visual Recognition [LADE] Links: arXiv\n Experimental Setup\n   Dataset Architecture     CIFAR-LT 100 ResNet-32   ImageNet-LT ResNeXt-50   Places-LT ResNet-152   iNaturalist18 ResNet-50      Summary  (Problem) Consider it as label shift problem - Unmatched source and target label distribution  (Solution) Disentangle source label distro and model prediction  After training [PC Softmax]: Baseline - Match model prediction to target label distro post-hoc  Post-Compensated (PC) Softmax   But how do we know the $p_t$ ? They assume three different distrubtion (Forward, Uniform, Backward) and show that PC Softmax performs better in all scenarios irrespective of the actual $p_t$ of the dataset.      During training [LADE]: Disentangle model prediction and source label distro  LADER loss : Disentangles logits from source label distro  LADE-CE loss : Match logits to target label distro  LADE = LADE-CE + $\\alpha$ LADER         MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition Links: arXiv\n Experimental Setup\n   Dataset Architecture     CIFAR-LT 10/100 ResNet-32   ImageNet-LT ResNet-50   iNaturalist 17/18 ResNet-50      Summary  (Old work) Implicit Semantic Data Augmentation (ISDA) : Enables class identity preserving augmentations by translating the features towards a meaningful semantic direction, using class covariance matrices.  (Problem) Scarce data of the tail classes would make it hard to get a good covarince matrix.  (Solution) MetaSAug : Meta learn the class covarince matrices using a balanced validation set.     MetaSAug Algorithm:  Studying the Covariance matrices obtained by the method to confirm whether its actually helping.  Do SVD on the Convariance matrix under various scenario and check the top 5 singluar values. Blue (others) : The singular value is concentrated on the just the first value. Red (Proposed) : The singular value is comparitivly spread out across all the top 5 values.       Experimental Setup\n   Task Dataset Architecture     Classification ImageNet-LT ResNet or ResNeXt-{50,101,152}   Classification Places-LT ResNet-{50,101,152}   Classification iNaturalist18 ResNet-{50,101,152}   Object detection, Instance segmentation LVIS [ResNet-{50} AND Mask R-CNN+FPN] OR [[ResNet-{50,101} or ResNeXt-{101}] AND Cascade R-CNN]   Semantic segmentation ADE20k [ResNet-{50,101} OR ResNeSt-{101}] AND [FCN OR DeepLabV3+]      ##### Summary - ### ICLR 21 #### [RIDE: Long-tailed Recognition by Routing Diverse Distribution-Aware Experts](https://openreview.net/forum?id=D9I3drBz4UC)  Datasets\n CIFAR-LT 100 ImageNet-LT iNaturalist18    ##### Summary - Multiple experts - Reduces model variance - Use multiple experts (networks) with shared earlier layers - Distribution aware diversity loss - Reduces model bias - Loss term that maximizes KL-div between experts - Dynamic expert routing - Reduces computational cost - Instead of using all the experts, choose experts depending on the sample at hand - Self-distillation - Can also do self distillation from a model with more experts to fewer experts -- ","date":1633919212,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633919212,"objectID":"113e74a9368a724e74f586cdfe58bbf4","permalink":"/post/long-tail-classification/","publishdate":"2021-10-11T07:56:52+05:30","relpermalink":"/post/long-tail-classification/","section":"post","summary":"Summaries of latest long-tail classification related papers.","tags":["summary","longtail","classification","computervision"],"title":"Long Tail Classification 📉","type":"post"},{"authors":["Rahul Vigneswaran"],"categories":["CheatSheets"],"content":"Table of Contents  GPU CPU Storage General Training  Loss Dataset/Dataloader Misc     GPU  If you have limited GPU memory, always lazy load the data. Instead of loading the images of the entire dataset at the same time, load the images batch by batch.  Check this thread for an example.   Batch size plays an important role in your training, so if you have a limited GPU memory and can\u0026rsquo;t fit the entire batch in it, do gradient accumulation. In this you do optimizer.step and model.zero_grad() once in few steps.  Check this link.   If you have GPU bottleneck and decide to go with torch.nn.DataParallel, there is a catch regarding the batch size.  Look at the thread below for more. I trained a model on 1 GPU and then using @PyTorch DataParallel on 2 GPUs. Even though I fixed the seed, I got 2 different training plots. Turns out, if you use batchsize=n and GPU_count=x, it\u0026#39;s equivalent to having effective_batchsize=n*x.\n\u0026mdash; Rahul Vigneswaran (@somethingmyname) June 2, 2021     Useful utilities/commands:  gpustat --watch glances    CPU  If you are running multiple experiments but have limited number of cores, use taskset --cpu-list \u0026lt;starting_thread\u0026gt;-\u0026lt;ending thread number\u0026gt; \u0026lt;your_code\u0026gt;.py. This will make sure your specific runs use only the allotted threads from \u0026lt;starting_thread\u0026gt;to \u0026lt;ending thread number\u0026gt; and prevents from constant reallocation of CPU threads as each run fight for the threads. Note that this is helpful only if everyone on the server respects the core allotment. More num_workers doesn\u0026rsquo;t lead to a faster data loader. In fact, in most cases having higher num_workers will lead to a slower data loader. As far as I know, there is no thumb rule but there does exist a sweet spot that is mostly identified through trial and error.  Check this thread for more.   Useful utilities/commands:  htop glances    Storage  Make sure you are running (read, log, train) on SSD. HDD causes I/O bottlenecks which are hard to get over even if you sell your soul to satan.  Check with this lsblk -o NAME,MOUNTPOINT,MODEL,ROTA,SIZE. ROTA == 0 means, the drive is an SSD.   Useful utilities/commands:  ncdu df -h    General Training Loss  If you have implemented a new type of loss, do a overfit test first. Instead of running the experiments on the entire dataset, overfit a single batch. You should be able to get your lower bound of the implemented loss and 100% train accuracy, else something is wrong with the implementation.  Check Karpathy\u0026rsquo;s blogpost.   nan related issues:  If you have some custom loss and don\u0026rsquo;t know where the nan is coming from, use PyTorch\u0026rsquo;s anamoly detection feature. Ways to tackle nan and related issues:       Dataset/Dataloader  If you want to know whether that idea that\u0026rsquo;s keeping you awake at night works but ImageNet takes too much time, then check these datasets:   Tiny ImageNet : 200 class version of ImageNet  mini ImageNet : 100 class version of ImageNet  Note that the above version is for few-shot learning. You can convert it into normal classification using this -     Imagenette : Easier 10 class version of ImageNet   Noisy Imagenette    Imagewoof: Relatively hard version of Imagenette   Noisy Imagewoof    Imagewang: Semi-supervised version which combines both Imagenette and Imagewoof    Misc  Implement resume functionality ASAP. Trust me, this will prevent crying yourself to sleep at night.  If you are using wandb, then you can even resume the logging. Feels like magic. Check the thread below for more. Coming from the world of CIFARs, I have never bothered to implement a resume function in my code. The Big boi datasets finally forced me to write a resume function today. I know exactly how it works but it still feels like magic to resume a large run after it crashes! pic.twitter.com/i6uInpIpXx\n\u0026mdash; Rahul Vigneswaran (@somethingmyname) June 2, 2021     If you come from the happy land of CIFARs and MNISTs like me, don\u0026rsquo;t stare at the runs. It will take days to weeks. Get a hobby or it\u0026rsquo;s finally time to open that \u0026ldquo;Interesting Papers\u0026rdquo; folder. As a final note, make sure to avoid everything in the thread below by Karpathy. most common neural net mistakes: 1) you didn\u0026#39;t try to overfit a single batch first. 2) you forgot to toggle train/eval mode for the net. 3) you forgot to .zero_grad() (in pytorch) before .backward(). 4) you passed softmaxed outputs to a loss that expects raw logits. ; others? :)\n\u0026mdash; Andrej Karpathy (@karpathy) July 1, 2018    ","date":1633890975,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633890975,"objectID":"eb6430fdd944b3aa7c2895d03cfcde72","permalink":"/post/working-with-large-datasets-tips-tricks/","publishdate":"2021-10-11T00:06:15+05:30","relpermalink":"/post/working-with-large-datasets-tips-tricks/","section":"post","summary":"A collection of things I learnt from working with large datasets.","tags":[],"title":"Tips \u0026 Tricks: Working With Large Datasets 🔢","type":"post"},{"authors":["Adepu Ravi Shankar","Yash Khasbage","Rahul Vigneswaran","Vineeth N. Balasubramanian"],"categories":[],"content":"","date":1597843123,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597843123,"objectID":"25d16180c83d90d8480b474f4ebffbfd","permalink":"/publication/layer-hessian-regularizer/","publishdate":"2020-08-19T18:48:43+05:30","relpermalink":"/publication/layer-hessian-regularizer/","section":"publication","summary":"Loss landscape analysis is extremely useful for a deeper understanding of the generalization ability of deep neural network models. In this work, we propose a layerwise loss landscape analysis where the loss surface at every layer is studied independently and also on how each correlates to the overall loss surface. We study the layerwise loss landscape by studying the eigenspectra of the Hessian at each layer. In particular, our results show that the layerwise Hessian geometry is largely similar to the entire Hessian. We also report an interesting phenomenon where the Hessian eigenspectrum of middle layers of the deep neural network are observed to most similar to the overall Hessian eigenspectrum. We also show that the maximum eigenvalue and the trace of the Hessian (both full network and layerwise) reduce as training of the network progresses. We leverage on these observations to propose a new regularizer based on the trace of the layerwise Hessian. Penalizing the trace of the Hessian at every layer indirectly forces Stochastic Gradient Descent to converge to flatter minima, which are shown to have better generalization performance. In particular, we show that such a layerwise regularizer can be leveraged to penalize the middlemost layers alone, which yields promising results. Our empirical studies on well-known deep nets across datasets support the claims of this work.","tags":[],"title":"A Deeper Look at the Hessian Eigen spectrum of Deep Neural Networks and its Applications to Regularization","type":"publication"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":" The theoretical literature on continual learning is minimal, and in this work, we try to fix the same. Worked on understanding how catastrophic forgetting translates to loss landscape and how it can pave the way for more theoretically grounded continual learning methods. Worked on understanding how regularization based catastrophic forgetting mitigation techniques behave in loss landscapes and why some methods are more successful in achieving the same than others. Analyzed whether tools like, to name a few - Hessian\u0026rsquo;s Eigen Spectrum, fluctuation-dissipation, intrinsic dimensionality can aid in understanding continual learning better.  Advisor :\n  Dr Vineeth N Balasubramanian  ","date":1597841046,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597841046,"objectID":"c9fa8adf05a4e81b3d908de3b1840f5c","permalink":"/project/incremental-loss-landscape/","publishdate":"2020-08-19T18:14:06+05:30","relpermalink":"/project/incremental-loss-landscape/","section":"project","summary":"Explores the continual/incremental learning setting in Deep learning from the perspective of loss landscapes.","tags":["research"],"title":"Incremental Loss Landscape","type":"project"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":" Recent works1 2 have demonstrated a bulk and outlier trend in their Hessian\u0026rsquo;s Eigen Spectrum. In this work, we have discovered a similar trend in the layer-wise spectrum, too, which indicates an implicit similarity between the overall loss landscape and layer-wise loss landscape, which is a community first. We leverage this observation and formulate a regularizer that forces the optimizer to converge to a minima of better generalization properties. Further, through this analysis, we have substantiated that studying the layer-wise loss landscape is worth the community\u0026rsquo;s efforts.   Work accepted at AAAI 2021.\n Advisor :\n  Dr Vineeth N Balasubramanian     Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians \u0026#x21a9;\u0026#xfe0e;\n  Empirical Analysis of the Hessian of Over-Parametrized Neural Networks \u0026#x21a9;\u0026#xfe0e;\n   ","date":1579437262,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579437262,"objectID":"3ecfd0061f8a8a726332d428331fbeb9","permalink":"/project/layerwise-hessian-analysis/","publishdate":"2020-01-19T18:04:22+05:30","relpermalink":"/project/layerwise-hessian-analysis/","section":"project","summary":"Understanding and leveraging the properties of layerwise losslandscape in conjunction with the overall loss landscape.","tags":["research"],"title":"Layerwise Hessian Analysis","type":"project"},{"authors":["Rahul Vigneswaran","Sachin-Kumar S","Neethu Mohan","Soman KP"],"categories":[],"content":"","date":1572289282,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572289282,"objectID":"bb99370102bac471d5c0e5d239d02bae","permalink":"/publication/dmd-based-feature-for-image-classification/","publishdate":"2020-03-29T00:31:22+05:30","relpermalink":"/publication/dmd-based-feature-for-image-classification/","section":"publication","summary":"Irrespective of the fact that Machine learning has produced groundbreaking results, it demands an enormous amount of data in order to perform so. Even though data production has been in its all-time high, almost all the data is unlabelled, hence making them unsuitable for training the algorithms. This paper proposes a novel method of extracting the features using Dynamic Mode Decomposition (DMD). The experiment is performed using data samples from Imagenet. The learning is done using SVM-linear, SVM-RBF, Random Kitchen Sink approach (RKS). The results have shown that DMD features with RKS give competing results.","tags":["limited-data","dmd"],"title":"Dynamic Mode Decomposition based feature for Image Classification","type":"publication"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":" Worked on understanding the existing techniques used for learning with limited labeled data and explored non-conventional techniques for efficiently learning a distribution with limited-resource. Used Dynamic Mode Decomposition (DMD) to extract the dominant features of images for classifying with limited labeled data.   Accepted for an oral presentation at TENCON\u0026rsquo;19.\n Advisors :\n  Dr Soman KP  Mr Sachin Kumar S  ","date":1560948599,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560948599,"objectID":"e5827769f723e2a72cb24e163a228de3","permalink":"/project/learning-with-limited-labeled-data/","publishdate":"2019-06-19T18:19:59+05:30","relpermalink":"/project/learning-with-limited-labeled-data/","section":"project","summary":"Using the concept of Dynamic Mode Decomposition from the field of Fluid Dynamics in a classification setting with limited labeled data.","tags":["academic"],"title":"Learning With Limited Labeled Data","type":"project"},{"authors":["Rahul Vigneswaran","Neethu Mohan","Soman KP"],"categories":[],"content":"","date":1555960755,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555960755,"objectID":"eae6444277c5bae38720240c5ef5d59c","permalink":"/publication/data-drive-computing-for-elasticity-via-chebyshev-approximation/","publishdate":"2020-03-29T00:49:15+05:30","relpermalink":"/publication/data-drive-computing-for-elasticity-via-chebyshev-approximation/","section":"publication","summary":"This paper proposes a data-driven approach for computing elasticity by means of a non-parametric regression approach rather than an optimization approach. The Chebyshev approximation is utilized for tackling the material data-sets non-linearity of the elasticity. Also, additional efforts have been taken to compare the results with several other state-of-the-art methodologies.","tags":["chebfun","data-driven","elasticity","mechanical"],"title":"Data Drive Computing for Elasticity via Chebyshev Approximation","type":"publication"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":" Explored the idea of Data-driven shape optimization, especially in ship hulls. Used Proper Orthogonal Decomposition based model order reduction approach and Dynamic Mode Decomposition (DMD) to reduce the time of turbulent flow simulation involved.  Advisors :\n  Dr Soman KP  Dr Gopalakrishnan EA  ","date":1555676516,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555676516,"objectID":"fe3a5f37189cae60d62482f1dcafeaef","permalink":"/project/shape-optimization/","publishdate":"2019-04-19T17:51:56+05:30","relpermalink":"/project/shape-optimization/","section":"project","summary":"Exploring ways to reduce the time of turbulent flow simulation in shape optimization of ship hulls.","tags":["research"],"title":"Shape Optimization using DMD and POD","type":"project"},{"authors":["Rahul Vigneswaran","Prabaharan Poornachandran","Soman KP"],"categories":[],"content":"","date":1554493509,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554493509,"objectID":"9dbc95faac51a6609a5f304d5d91c21a","permalink":"/publication/a-compendium-on-network-and-host-based-intrusion-detection-systems/","publishdate":"2020-03-29T01:15:09+05:30","relpermalink":"/publication/a-compendium-on-network-and-host-based-intrusion-detection-systems/","section":"publication","summary":"The techniques of deep learning have become the state of the art methodology for executing complicated tasks from various domains of computer vision, natural language processing, and several other areas. Due to its rapid development and promising benchmarks in those fields, researchers started experimenting with this technique to perform in the area of, especially in intrusion detection related tasks. Deep learning is a subset and a natural extension of classical Machine learning and an evolved model of neural networks. This paper contemplates and discusses all the methodologies related to the leading edge Deep learning and Neural network models purposing to the arena of Intrusion Detection Systems.","tags":["cyber-security","intrusion-detection-system"],"title":"A Compendium on Network and Host Based Intrusion Detection Systems","type":"publication"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":" Conducted a detailed study on various contents of the water samples, especially trace metals, which were collected Pre and post to a Flood in the state of Kerala.  Advisor :\n  Ms Geena Prasad  ","date":1545223618,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545223618,"objectID":"3e5a70dc443e545239ab1f666bce69c7","permalink":"/project/trace-metal/","publishdate":"2018-12-19T18:16:58+05:30","relpermalink":"/project/trace-metal/","section":"project","summary":"Analysing the trace metal content in pre and post flood conditions.","tags":["academic"],"title":"Trace metal analysis of Pre-flood and Post-flood drinking water in Kerala","type":"project"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":" Implemented and Contrasted Deep and Shallow Neural Nets in the Cybersecurity use case of Intrusion Detection Systems (IDS) while studying the various SOTAs of Host and Network-based Intrusion Detection Systems (IDS).   Work accepted at a SCOPUS indexed conference held at IISC Banglore.\n Advisors :\n  Dr Soman KP  Dr Prabaharan Poornachandran  ","date":1535489790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535489790,"objectID":"b36e0d5efd1183728d79d709482624c0","permalink":"/project/intrusion-detection-systems/","publishdate":"2018-08-29T02:26:30+05:30","relpermalink":"/project/intrusion-detection-systems/","section":"project","summary":"Understanding the effects of deep and shallow neural networks in Intrusion Detection Systems (IDS).","tags":["research"],"title":"Intrusion Detection Systems","type":"project"},{"authors":["Rahul Vigneswaran","Vinayakumar R","Soman KP","Prabaharan Poornachandran"],"categories":[],"content":"","date":1531165904,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531165904,"objectID":"554f733208902b342894c89bb4102198","permalink":"/publication/evaluating-shallow-and-deep-neural-networks-for-network-intrusion-detection-systems-in-cyber-security/","publishdate":"2020-03-29T01:21:44+05:30","relpermalink":"/publication/evaluating-shallow-and-deep-neural-networks-for-network-intrusion-detection-systems-in-cyber-security/","section":"publication","summary":"Intrusion detection system (IDS) has become an essential layer in all the latest ICT system due to an urge towards cyber safety in the day-to-day world. Reasons including uncertainty in ﬁnding the types of attacks and increased the complexity of advanced cyber attacks, IDS calls for the need of integration of Deep Neural Networks (DNNs). In this paper, DNNs have been utilized to predict the attacks on Network Intrusion Detection System (N-IDS). A DNN with 0.1 rate of learning is applied and is run for 1000 number of epochs and KDDCup-’99’ dataset has been used for training and benchmarking the network. For comparison purposes, the training is done on the same dataset with several other classical machine learning algorithms and DNN of layers ranging from 1 to 5. The results were compared and concluded that a DNN of 3 layers has superior performance over all the other classical machine learning algorithms.","tags":["cyber-security","intrusion-detection-system"],"title":"Evaluating Shallow and Deep Neural Networks for Network Intrusion Detection Systems in Cyber Security","type":"publication"},{"authors":null,"categories":null,"content":"\r[Oct 2021] : Work with Makarand Tapaswi, Marc Law and Vineeth NB, titled TailCalibX : Feature generation for Long-tailed Classification got accepted into ICVGIP 2021.\n[Dec 2020] : Work with Vineeth N Balasubramanian, titled A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to Regularization got accepted into AAAI 2021.\n[Oct 2020] : Started working with Makarand Tapaswi from Wadhwani AI on problems related to Long-tail distribution.\n[July 2019] : Joined as a Research Intern under Dr Vineeth N Balasubramanian at IIT Hyderabad\u0026lsquo;s Machine Learning and Vision Lab.\n","date":1512066600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512066600,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"/news/","publishdate":"2017-12-01T00:00:00+05:30","relpermalink":"/news/","section":"","summary":"\r\nList of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":["Rahul Vigneswaran"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a3840dbe757a8c996765208907179245","permalink":"/implementations/patch-up/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/implementations/patch-up/","section":"implementations","summary":"Loss landscape analysis is extremely useful for a deeper understanding of the generalization ability of deep neural network models. In this work, we propose a layerwise loss landscape analysis where the loss surface at every layer is studied independently and also on how each correlates to the overall loss surface. We study the layerwise loss landscape by studying the eigenspectra of the Hessian at each layer. In particular, our results show that the layerwise Hessian geometry is largely similar to the entire Hessian. We also report an interesting phenomenon where the Hessian eigenspectrum of middle layers of the deep neural network are observed to most similar to the overall Hessian eigenspectrum. We also show that the maximum eigenvalue and the trace of the Hessian (both full network and layerwise) reduce as training of the network progresses. We leverage on these observations to propose a new regularizer based on the trace of the layerwise Hessian. Penalizing the trace of the Hessian at every layer indirectly forces Stochastic Gradient Descent to converge to flatter minima, which are shown to have better generalization performance. In particular, we show that such a layerwise regularizer can be leveraged to penalize the middlemost layers alone, which yields promising results. Our empirical studies on well-known deep nets across datasets support the claims of this work.","tags":[],"title":"PatchUp: A Regularization Technique for Convolutional Neural Networks","type":"implementations"}]