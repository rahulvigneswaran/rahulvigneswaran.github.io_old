<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Rahul Vigneswaran</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2021 Rahul Vigneswaran Â©</copyright><lastBuildDate>Mon, 11 Oct 2021 07:56:52 +0530</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:square]</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Long Tail Classification ðŸ“‰</title>
      <link>/post/long-tail-classification/</link>
      <pubDate>Mon, 11 Oct 2021 07:56:52 +0530</pubDate>
      <guid>/post/long-tail-classification/</guid>
      <description>&lt;!-- Template

&lt;!-- #### []() 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Datasets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CIFAR-LT 100&lt;/li&gt;
&lt;li&gt;ImageNet-LT&lt;/li&gt;
&lt;li&gt;Places-LT&lt;/li&gt;
&lt;li&gt;iNaturalist18&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;


##### Summary --&gt;
&lt;!-- #### []() 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Experimental Setup&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Architecture&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CIFAR-LT 100&lt;/td&gt;
&lt;td&gt;ResNet-32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ImageNet-LT&lt;/td&gt;
&lt;td&gt;ResNeXt-50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Places-LT&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iNaturalist18&lt;/td&gt;
&lt;td&gt;ResNeXt-50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  &lt;/div&gt;
&lt;/div&gt;


##### Summary --&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Updates&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[Oct 21] Started adding CVPR, ICLR 21 papers.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#2021&#34;&gt;2021&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#cvpr-21&#34;&gt;CVPR 21&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;2021&#34;&gt;2021&lt;/h2&gt;
&lt;h3 id=&#34;cvpr-21&#34;&gt;CVPR 21&lt;/h3&gt;
&lt;h4 id=&#34;improving-calibration-for-long-tailed-recognitionhttpsarxivorgpdf210400466pdf&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2104.00466.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Calibration for Long-Tailed Recognition&lt;/a&gt;&lt;/h4&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Datasets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CIFAR-LT 10/100&lt;/li&gt;
&lt;li&gt;ImageNet-LT&lt;/li&gt;
&lt;li&gt;Places-LT&lt;/li&gt;
&lt;li&gt;iNaturalist18&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h5 id=&#34;summary&#34;&gt;Summary&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;High Expected Calibration Error (ECE) for networks trained on LT datasets
&lt;ul&gt;
&lt;li&gt;Mix-up &lt;em&gt;(Takes care of CNN)&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Mixup in 1st stage of decouple helps&lt;/li&gt;
&lt;li&gt;Reduces the weight norm of head and increases tail class weight norm
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;2021-11-11-23-56-26.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Label aware smoothing &lt;em&gt;(Takes care of Classifier)&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;Higher smoothing factor for head and lower for tail
&lt;ul&gt;
&lt;li&gt;They try three types - Concave, Linear, Convex
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;2021-11-11-23-57-45.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cRT has better representation ability and LWS has better generalization property
&lt;ul&gt;
&lt;li&gt;They combine both together in the second stage&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;2021-11-12-00-04-54.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Problem)&lt;/strong&gt; Dataset bias/domain shift when decoupling
&lt;ul&gt;
&lt;li&gt;In 2-stage training, stage-1 is trained on a different domain (sampler) than stage-2 -&amp;gt; Domain shift.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Solution)&lt;/strong&gt; Batch Normalization
&lt;ul&gt;
&lt;li&gt;During stage 2, update the running mean and variance but donâ€™t change the trainable params in BN layer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Notes:
&lt;ul&gt;
&lt;li&gt;cRT: &lt;img src=&#34;2021-11-12-00-11-28.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;li&gt;LWS&lt;img src=&#34;2021-11-12-00-12-37.png&#34; alt=&#34;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;Works better for large scale datasets compared to cRT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;contrastive-learning-based-hybrid-networks-for-long-tailed-image-classificationhttpsarxivorgabs210314267&#34;&gt;
&lt;a href=&#34;https://arxiv.org/abs/2103.14267&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification&lt;/a&gt;&lt;/h4&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Experimental Setup&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Architecture&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CIFAR-LT 10/100&lt;/td&gt;
&lt;td&gt;ResNet-32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;iNaturalist18&lt;/td&gt;
&lt;td&gt;ResNet-50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h5 id=&#34;summary-1&#34;&gt;Summary&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Main strategy:
&lt;ul&gt;
&lt;li&gt;Pull similar samples together AND Push dissimilar samples apart (Supervised contrastive learning (SC))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hybrid network (Transition from 1 -&amp;gt; 2 with a curriculum) (!! Is this not similar to BBN? Yes but with two different loss instead of samplers) &lt;img src=&#34;2021-11-12-00-42-30.png&#34; alt=&#34;&#34;&gt;
&lt;ol&gt;
&lt;li&gt;Features -&amp;gt; Constrastive Loss  (SC)
&lt;ul&gt;
&lt;li&gt;Input : Labels are either positive or negative.&lt;/li&gt;
&lt;li&gt;Backbone : Common ResNet&lt;/li&gt;
&lt;li&gt;Projection head : Non-linear embedding layer (with 1 hidden layer) -&amp;gt; $l_2$ normalize the embedding&lt;/li&gt;
&lt;li&gt;Loss : Supervised contrastive learning loss ($L_{SCL}$)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(Problem)&lt;/strong&gt; Extra memory consumption problem : We need to use both distance to the +ves and -ves. The number -ves would be high which makes it essential to keep alot of information in the memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Solution)&lt;/strong&gt; Propose Prototypical supervised constrastive learning (PSC)
&lt;ul&gt;
&lt;li&gt;Pull towards prototypes of its own class and push away from the prototypes of all other classes.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multiple Prototype Contrastive Learning&lt;/strong&gt; (MPSC): While PSC uses only one prototype per class. (BUT THEY DONT SHOW ANY RESULTS ON THIS! Future work.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Classifier -&amp;gt; Cross-entropy  (CE)
&lt;ul&gt;
&lt;li&gt;Input: Usual labels&lt;/li&gt;
&lt;li&gt;Backbone : Common ResNet&lt;/li&gt;
&lt;li&gt;Projection head : Linear embedding layer (no hidden layer)&lt;/li&gt;
&lt;li&gt;Loss : Cross Entropy ($L_{CE}$)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--     
#### [Disentangling Label Distribution for Long-tailed Visual Recognition [LADE]](https://arxiv.org/abs/2012.00321)

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Datasets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CIFAR-LT 100&lt;/li&gt;
&lt;li&gt;ImageNet-LT&lt;/li&gt;
&lt;li&gt;Places-LT&lt;/li&gt;
&lt;li&gt;iNaturalist18&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;


##### Summary
- Consider it as label shift problem - Unmatched source and target label distribution
  - Disentangle source label distro and model prediction
    - After training [PC Softmax]: Baseline - Match model prediction to target label distro post-hoc
      - Post-Compensated (PC) Softmax
    - During training [LADE]: Disentangle model prediction and source label distro 
      - LADER loss disentangles logits from source label distro
      - Use LADE-CE loss to match logits to target label distro
      - LADE = LADE-CE + LADER



#### [Distribution Alignment: A Unified Framework for Long-tail Visual Recognition](https://arxiv.org/abs/2103.16370) 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Experimental Setup&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Architecture&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Classification&lt;/td&gt;
&lt;td&gt;ImageNet-LT&lt;/td&gt;
&lt;td&gt;ResNet or ResNeXt-{50,101,152}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classification&lt;/td&gt;
&lt;td&gt;Places-LT&lt;/td&gt;
&lt;td&gt;ResNet-{50,101,152}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Classification&lt;/td&gt;
&lt;td&gt;iNaturalist18&lt;/td&gt;
&lt;td&gt;ResNet-{50,101,152}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Object detection, Instance segmentation&lt;/td&gt;
&lt;td&gt;LVIS&lt;/td&gt;
&lt;td&gt;[ResNet-{50} AND Mask R-CNN+FPN] OR [[ResNet-{50,101} or ResNeXt-{101}] AND Cascade R-CNN]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Semantic segmentation&lt;/td&gt;
&lt;td&gt;ADE20k&lt;/td&gt;
&lt;td&gt;[ResNet-{50,101} OR ResNeSt-{101}]  AND [FCN OR DeepLabV3+]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
  &lt;/div&gt;
&lt;/div&gt;

##### Summary
- 

### ICLR 21
#### [RIDE: Long-tailed Recognition by Routing Diverse Distribution-Aware Experts](https://openreview.net/forum?id=D9I3drBz4UC) 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Datasets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CIFAR-LT 100&lt;/li&gt;
&lt;li&gt;ImageNet-LT&lt;/li&gt;
&lt;li&gt;iNaturalist18&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;


##### Summary
- Multiple experts - Reduces model variance
  - Use multiple experts (networks) with shared earlier layers
- Distribution aware diversity loss - Reduces model bias
  - Loss term that maximizes KL-div between experts
- Dynamic expert routing - Reduces computational cost
  - Instead of using all the experts, choose experts depending on the sample at hand
  - Self-distillation
      - Can also do self distillation from a model with more experts to fewer experts --&gt;
</description>
    </item>
    
    <item>
      <title>Tips &amp; Tricks: Working With Large Datasets ðŸ”¢</title>
      <link>/post/working-with-large-datasets-tips-tricks/</link>
      <pubDate>Mon, 11 Oct 2021 00:06:15 +0530</pubDate>
      <guid>/post/working-with-large-datasets-tips-tricks/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#gpu&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#cpu&#34;&gt;CPU&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#storage&#34;&gt;Storage&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#general-training&#34;&gt;General Training&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#loss&#34;&gt;Loss&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#datasetdataloader&#34;&gt;Dataset/Dataloader&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;gpu&#34;&gt;GPU&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you have limited GPU memory, always lazy load the data. Instead of loading the images of the entire dataset at the same time, load the images batch by batch.
&lt;ul&gt;
&lt;li&gt;Check 
&lt;a href=&#34;https://discuss.pytorch.org/t/loading-huge-data-functionality/346/3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this thread&lt;/a&gt; for an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Batch size plays an important role in your training, so if you have a limited GPU memory and can&amp;rsquo;t fit the entire batch in it, do gradient accumulation. In this you do &lt;code&gt;optimizer.step&lt;/code&gt; and &lt;code&gt;model.zero_grad()&lt;/code&gt; once in few steps.
&lt;ul&gt;
&lt;li&gt;Check 
&lt;a href=&#34;https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this link&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you have GPU bottleneck and decide to go with &lt;code&gt;torch.nn.DataParallel&lt;/code&gt;, there is a catch regarding the batch size.
&lt;ul&gt;
&lt;li&gt;Look at the thread below for more. &lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I trained a model on 1 GPU and then using &lt;a href=&#34;https://twitter.com/PyTorch?ref_src=twsrc%5Etfw&#34;&gt;@PyTorch&lt;/a&gt; DataParallel on 2 GPUs. Even though I fixed the seed, I got 2 different training plots. &lt;br&gt;&lt;br&gt;Turns out, if you use batchsize=n and GPU_count=x, it&amp;#39;s equivalent to having effective_batchsize=n*x.&lt;/p&gt;&amp;mdash; Rahul Vigneswaran (@somethingmyname) &lt;a href=&#34;https://twitter.com/somethingmyname/status/1400042667543654402?ref_src=twsrc%5Etfw&#34;&gt;June 2, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Useful utilities/commands:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;gpustat --watch&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glances&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cpu&#34;&gt;CPU&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If you are running multiple experiments but have limited number of cores, use &lt;code&gt;taskset --cpu-list &amp;lt;starting_thread&amp;gt;-&amp;lt;ending thread number&amp;gt; &amp;lt;your_code&amp;gt;.py&lt;/code&gt;. This will make sure your specific runs use only the allotted threads from &lt;code&gt;&amp;lt;starting_thread&amp;gt;&lt;/code&gt;to &lt;code&gt;&amp;lt;ending thread number&amp;gt;&lt;/code&gt; and prevents from constant reallocation of CPU threads as each run fight for the threads. Note that this is helpful only if everyone on the server respects the core allotment.&lt;/li&gt;
&lt;li&gt;More &lt;code&gt;num_workers&lt;/code&gt; doesn&amp;rsquo;t lead to a faster data loader. In fact, in most cases having higher &lt;code&gt;num_workers&lt;/code&gt; will lead to a slower data loader. As far as I know, there is no thumb rule but there does exist a sweet spot that is mostly identified through trial and error.
&lt;ul&gt;
&lt;li&gt;Check 
&lt;a href=&#34;https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this thread&lt;/a&gt; for more.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Useful utilities/commands:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;htop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glances&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;storage&#34;&gt;Storage&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Make sure you are running (read, log, train) on SSD. HDD causes I/O bottlenecks which are hard to get over even if you sell your soul to satan.
&lt;ul&gt;
&lt;li&gt;Check with this &lt;code&gt;lsblk -o NAME,MOUNTPOINT,MODEL,ROTA,SIZE&lt;/code&gt;. ROTA == 0 means, the drive is an SSD.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Useful utilities/commands:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ncdu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;df -h&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;general-training&#34;&gt;General Training&lt;/h2&gt;
&lt;h3 id=&#34;loss&#34;&gt;Loss&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you have implemented a new type of loss, do a overfit test first. Instead of running the experiments on the entire dataset, overfit a single batch. You should be able to get your lower bound of the implemented loss and 100% train accuracy, else something is wrong with the implementation.
&lt;ul&gt;
&lt;li&gt;Check 
&lt;a href=&#34;http://karpathy.github.io/2019/04/25/recipe/#:~:text=overfit%20one%20batch.%20Overfit%20a%20single%20batch,we%20cannot%20continue%20to%20the%20next%20stage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Karpathy&amp;rsquo;s blogpost&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nan&lt;/code&gt; related issues:
&lt;ul&gt;
&lt;li&gt;If you have some custom loss and don&amp;rsquo;t know where the &lt;code&gt;nan&lt;/code&gt; is coming from, use 
&lt;a href=&#34;https://pytorch.org/docs/master/autograd.html#anomaly-detection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch&amp;rsquo;s anamoly detection feature&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Ways to tackle &lt;code&gt;nan&lt;/code&gt; and related issues: 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/XlYD8jn1ayE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datasetdataloader&#34;&gt;Dataset/Dataloader&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you want to know whether that idea that&amp;rsquo;s keeping you awake at night works but ImageNet takes too much time, then check these datasets:
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.kaggle.com/c/tiny-imagenet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tiny ImageNet&lt;/a&gt; : 200 class version of ImageNet&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/yaoyao-liu/mini-imagenet-tools&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mini ImageNet&lt;/a&gt; : 100 class version of ImageNet
&lt;ul&gt;
&lt;li&gt;Note that the above version is for few-shot learning. You can convert it into normal classification using this - &lt;insert link here&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/fastai/imagenette&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imagenette&lt;/a&gt; : Easier 10 class version of ImageNet
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/fastai/imagenette/tree/master/noisy_labels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noisy Imagenette&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/fastai/imagenette#imagewoof&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imagewoof&lt;/a&gt;: Relatively hard version of Imagenette
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/fastai/imagenette/tree/master/noisy_labels&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noisy Imagewoof&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/fastai/imagenette#image%E7%BD%91&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imagewang&lt;/a&gt;: Semi-supervised version which combines both Imagenette and Imagewoof&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;misc&#34;&gt;Misc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Implement resume functionality ASAP. Trust me, this will prevent crying yourself to sleep at night.
&lt;ul&gt;
&lt;li&gt;If you are using wandb, then you can even resume the logging. Feels like magic. Check the thread below for more. &lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Coming from the world of CIFARs, I have never bothered to implement a resume function in my code. The Big boi datasets finally forced me to write a resume function today. I know exactly how it works but it still feels like magic to resume a large run after it crashes! &lt;a href=&#34;https://t.co/i6uInpIpXx&#34;&gt;pic.twitter.com/i6uInpIpXx&lt;/a&gt;&lt;/p&gt;&amp;mdash; Rahul Vigneswaran (@somethingmyname) &lt;a href=&#34;https://twitter.com/somethingmyname/status/1400237720413171713?ref_src=twsrc%5Etfw&#34;&gt;June 2, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If you come from the happy land of CIFARs and MNISTs like me, don&amp;rsquo;t stare at the runs. It will take days to weeks. Get a hobby or it&amp;rsquo;s finally time to open that &amp;ldquo;Interesting Papers&amp;rdquo; folder.&lt;/li&gt;
&lt;li&gt;As a final note, make sure to avoid everything in the thread below by Karpathy. &lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;most common neural net mistakes: 1) you didn&amp;#39;t try to overfit a single batch first. 2) you forgot to toggle train/eval mode for the net. 3) you forgot to .zero_grad() (in pytorch) before .backward(). 4) you passed softmaxed outputs to a loss that expects raw logits. ; others? :)&lt;/p&gt;&amp;mdash; Andrej Karpathy (@karpathy) &lt;a href=&#34;https://twitter.com/karpathy/status/1013244313327681536?ref_src=twsrc%5Etfw&#34;&gt;July 1, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
